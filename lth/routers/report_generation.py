from fastapi import APIRouter
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain.chains.summarize import load_summarize_chain
from langchain.docstore.document import Document
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnableLambda
from dotenv import load_dotenv
from typing import List
import warnings
warnings.filterwarnings("ignore")


router = APIRouter(prefix="/report", tags=["Report Generation"])

load_dotenv()
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.1)

summary_prompt = PromptTemplate(
    template="ì˜¨ë¼ì¸ ê·¸ë£¨ë° ê°ì§€ ë³´ê³ ì„œ ì‘ì„±ì„ ìœ„í•´ ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”:\n{input}",
    input_variables=["input"],
)

report_prompt = PromptTemplate(
    template=(
        """
        ë‹¤ìŒ ëŒ€í™” ìš”ì•½ê³¼ í…œí”Œë¦¿ì„ ë°”íƒ•ìœ¼ë¡œ ë³´í˜¸ìì—ê²Œ ì „ë‹¬í•  ê°„ëµí•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.(**ì€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.)
        ìš”ì•½: {summary}

        "ì˜¨ë¼ì¸ ê·¸ë£¨ë°ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤"

        [ì˜¨ë¼ì¸ ê·¸ë£¨ë° ëŒ€í™” ì˜ì‹¬ ê°œìš”]
        - ë°œìƒ ë‚ ì§œ ë° ì‹œê°„: {date}
        - SNS í”Œë«í¼: {chat_platform}
        - ìƒëŒ€ë°© ê³„ì •: {predators_name}
        - ìë…€: {child_name}

        [ëŒ€í™” ìš”ì•½]

        [ê¶Œê³  ì‚¬í•­]

        [ëŒ€ì‘ ë°©ì•ˆ]
        
        [ì˜¨ë¼ì¸ ê·¸ë£¨ë° í”¼í•´ ì§€ì› ê¸°ê´€ ì •ë³´]
        - ë””ì§€í„¸ì„±ë²”ì£„í”¼í•´ìì§€ì›ì„¼í„°(02-734-8994 365ì¼ ìƒë‹´ê°€ëŠ¥) : í”¼í•´ì ‘ìˆ˜, ìƒë‹´, ìœ í¬ í˜„í™© ëª¨ë‹ˆí„°ë§, ìˆ˜ì‚¬Â·ë²•ë¥ Â·ì˜ë£Œ ì—°ê³„ ì§€ì› 
        - ì„œìš¸ë””ì§€í„¸ì„±ë²”ì£„ ì•ˆì‹¬ì§€ì›ì„¼í„°(02-815-0382/íœ´ì¼, ì•¼ê°„ 02-1366) : ìƒë‹´ì§€ì›, ìˆ˜ì‚¬ì§€ì›, ë²•ë¥ ì§€ì›, ì‹¬ë¦¬ìƒë‹´ì§€ì›, ì˜ë£Œ ì§€ì›, í”¼í•´ ì´¬ì˜ë¬¼ ì‚­ì œ ì§€ì›
        - ê²½ì°°ì²­ ì‚¬ì´ë²„ìˆ˜ì‚¬êµ­(ê¸´ê¸‰ì‹ ê³  112, 365ì¼ 24ì‹œê°„ ìƒë‹´): ì‚¬ì´ë²„ë²”ì£„ ì‹ ê³ ì‹œìŠ¤í…œ(ì‹ ê³ Â·ìƒë‹´Â·ì œë³´)
        - ì²­ì†Œë…„ì‚¬ì´ë²„ ìƒë‹´ì„¼í„°(ì²­ì†Œë…„ì „í™” 1388, 365ì¼ 24ì‹œê°„ ìƒë‹´): ì˜¨ë¼ì¸(ì±„íŒ…/ê²Œì‹œíŒ ë“±)Â·ì¹´ì¹´ì˜¤í†¡ ë° ë¬¸ì ìƒë‹´
        """
    ),
    input_variables=["summary", "date", "chat_platform", "predators_name", "child_name"],
)

class ChatText(BaseModel):
    text: List[str]

class ReportRequest(BaseModel):
    chat_text: ChatText
    date: str
    chat_platform: str
    predators_name: str
    child_name: str

@router.post("/generate/")
async def generate_report(request: ReportRequest):
    conversation_text = "\n".join(request.chat_text.text)
    document = Document(page_content=conversation_text)
    
    summary_chain = load_summarize_chain(llm, chain_type="stuff", prompt=summary_prompt, document_variable_name="input")
    summary = summary_chain.run([document])

    generate = RunnableLambda(lambda summary: llm.invoke(report_prompt.format(
        summary=summary,
        date=request.date,
        chat_platform=request.chat_platform,
        predators_name=request.predators_name,
        child_name=request.child_name,
    )))
    
    final_report = generate.invoke(summary)
    
    print("===============ì˜¨ë¼ì¸ ê·¸ë£¨ë° ì˜ì‹¬ ëŒ€í™” ë¦¬í¬íŠ¸===============")
    print("ğŸ“ ë³´ê³ ì„œ ë‚´ìš©:\n",final_report.content)
    return {"report": final_report.content}