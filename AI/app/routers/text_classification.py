from fastapi import APIRouter
from pydantic import BaseModel
import torch
from transformers import BertForSequenceClassification
from kobert_tokenizer import KoBERTTokenizer
from collections import deque
import os
import time

router = APIRouter(prefix="/text", tags=["Text Classification"])

# ëª¨ë¸ ë¡œë“œ
model_checkpoint = "../models/kobert_finetuning"
tokenizer = KoBERTTokenizer.from_pretrained("skt/kobert-base-v1")
model = BertForSequenceClassification.from_pretrained(model_checkpoint).to("cuda")
model.eval()

def preprocess(chat):
    return "[CLS] " + " [SEP] ".join(chat) + " [SEP]"

def tokenize_function(text):
    return tokenizer(
        text, add_special_tokens=False, max_length=512, truncation=True, padding="max_length", return_tensors="pt"
    )

# íƒì§€ ê¸°ì¤€ ë° ìœˆë„ìš° ì„¤ì •
skepticism = 5
window_num = 10
window_size = 10
detection_list = deque(maxlen=window_num)
chat_list = deque(maxlen=window_num)

class ChatText(BaseModel):
    text: list[str]

@router.post("/predict/")
async def predict(chat_text: ChatText):
    os.system('clear')
    chat_text = [c.replace('\u200b', '') for c in chat_text.text]
    print("===============ì‹¤ì‹œê°„ ì˜¨ë¼ì¸ ê·¸ë£¨ë° íƒì§€ì¤‘...===============")
    print('ğŸ’¬ í˜„ì¬ ìœˆë„ìš° ì±„íŒ… ë‚´ìš©:')
    for t in chat_text:
        if '[ìƒëŒ€ë°©]' in t:
            t = t.replace('[ìƒëŒ€ë°©]:', '')
            print('ğŸ‘¤:', t)
        else:
            t = t.replace('[ìë…€]:', '')
            print('ğŸ‘§:', t)
    print()
    chat = preprocess(chat_text)
    tokenized_input = tokenize_function(chat)

    inputs = tokenized_input["input_ids"].to("cuda")
    attention_mask = tokenized_input["attention_mask"].to("cuda")

    with torch.no_grad():
        output = model(inputs, attention_mask=attention_mask)
    
    prediction = torch.argmax(output.logits, dim=-1).cpu().item()
    detection_list.append(prediction)

    print(f'âœ… ìµœê·¼ {window_num}ê°œ ë‚´ ìœˆë„ìš° ì˜ˆì¸¡: {detection_list}')
    if prediction == 0:
        print('âœ… í˜„ì¬ ìœˆë„ìš° ì±„íŒ… ì˜ˆì¸¡: ğŸ˜‡')
    else:
        print('âœ… í˜„ì¬ ìœˆë„ìš° ì±„íŒ… ì˜ˆì¸¡: ğŸ˜ˆ')

    # ìœˆë„ìš° í¬ê¸°ë¥¼ ì´ˆê³¼í•  ê²½ìš° ì±„íŒ… ì €ì¥(ìµœê·¼ ìœˆë„ìš° ê°œìˆ˜-1 ë§Œí¼)
    if len(chat_text) == window_size:
        chat_list.append(chat_text[0])

    # íƒì§€ ê¸°ì¤€(skepticism) ì´ˆê³¼ ì‹œ
    if sum(detection_list) >= skepticism:
        print('âœ… ìµœì¢… ì˜ˆì¸¡: ğŸ˜ˆ\n')
        detection_list.clear()    # íƒì§€ ì‹œ deque ì´ˆê¸°í™”
        report_prompt = list(chat_list) + chat_text[1:]
        print('ğŸš¨ğŸš¨ğŸš¨ ì˜¨ë¼ì¸ ê·¸ë£¨ë°ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ğŸš¨ğŸš¨ğŸš¨')
        return {"prediction": 1, "chat_text": report_prompt}
    
    print('âœ… ìµœì¢… ì˜ˆì¸¡: ğŸ˜‡\n')
    return {"prediction": 0, "chat_text": None}