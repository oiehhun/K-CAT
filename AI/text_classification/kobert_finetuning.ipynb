{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k-cat/anaconda3/envs/lth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-26 17:29:17.930895: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-26 17:29:18.016298: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-26 17:29:18.036562: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 17:29:18.616039: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2025-02-26 17:29:18.616112: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2025-02-26 17:29:18.616118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AdamW, EarlyStoppingCallback\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertForSequenceClassification\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "\n",
    "import wandb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moiehhun\u001b[0m (\u001b[33moiehhun-yonsei-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/k-cat/users/lth/bert/wandb/run-20250209_181559-0y57bp2s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/oiehhun-yonsei-university/grooming/runs/0y57bp2s' target=\"_blank\">0209_run1</a></strong> to <a href='https://wandb.ai/oiehhun-yonsei-university/grooming' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/oiehhun-yonsei-university/grooming' target=\"_blank\">https://wandb.ai/oiehhun-yonsei-university/grooming</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/oiehhun-yonsei-university/grooming/runs/0y57bp2s' target=\"_blank\">https://wandb.ai/oiehhun-yonsei-university/grooming/runs/0y57bp2s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/oiehhun-yonsei-university/grooming/runs/0y57bp2s?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbc78ca0be0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='grooming', name='kobert_finetuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/text_data/train_data.csv')\n",
    "valid_data = pd.read_csv('../data/text_data/valid_data.csv')\n",
    "test_data = pd.read_csv('../data/text_data/test_data.csv')\n",
    "\n",
    "train_data = shuffle(train_data, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델 불러오기\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "model = BertForSequenceClassification.from_pretrained(\"skt/kobert-base-v1\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [2, 1185, 5400, 1457, 7835, 2125, 5898, 3156, 7083, 4204, 5405, 6855, 54, 3, 3097, 46, 1221, 5850, 1406, 2123, 6079, 2628, 2233, 2874, 2355, 5683, 2874, 3278, 55, 7347, 517, 6357, 6844, 54, 3, 1469, 5330, 2964, 1370, 5439, 3945, 5595, 3219, 7788, 517, 6751, 7083, 3169, 7303, 46, 1370, 2962, 4196, 3868, 55, 3, 2267, 5550, 3394, 5474, 6896, 1723, 7864, 6855, 54, 3, 3166, 5405, 6855, 46, 880, 1907, 54, 3, 3135, 5724, 517, 364, 365, 364, 3, 3135, 5724, 46, 3942, 4307, 258, 3, 1100, 6797, 54, 2267, 5550, 4045, 4384, 6896, 517, 6989, 6855, 54, 3, 3166, 5405, 6855, 46, 1370, 4299, 995, 5468, 3991, 2011, 3868, 54, 1469, 5330, 3301, 3879, 4205, 54, 3, 3093, 3166, 5405, 6855, 46, 2149, 6812, 46, 1457, 2267, 7848, 7788, 517, 6751, 7318, 3155, 5, 1370, 5859, 517, 5540, 54, 3, 3097, 6844, 1100, 6797, 46, 1457, 2705, 7788, 3868, 54, 3, 1406, 4996, 1469, 2123, 2224, 921, 1267, 5876, 54, 3, 4102, 7096, 6844, 258, 1375, 1469, 5330, 2186, 6488, 5771, 1435, 1174, 7396, 5400, 4930, 905, 832, 1544, 7245, 54, 3, 3612, 4996, 5, 1370, 2186, 6488, 7330, 1469, 2123, 2366, 921, 4996, 1267, 5876, 54, 3, 3093, 46, 1457, 4102, 1562, 7227, 7798, 629, 40, 3, 1189, 517, 6618, 7342, 7784, 1958, 7794, 862, 6844, 54, 3, 1457, 3459, 1423, 832, 54, 3, 1204, 1965, 7303, 6553, 993, 6141, 7018, 54, 1189, 1370, 5801, 5400, 3860, 862, 6844, 54, 1406, 1469, 5330, 4102, 4207, 54, 3, 1370, 5859, 1457, 4207, 7848, 54, 1457, 5760, 1435, 4102, 771, 3860, 2584, 7417, 1546, 5400, 4998, 7303, 55, 3, 771, 3863, 258, 2811, 6896, 46, 1457, 4368, 517, 6188, 7245, 54, 1457, 6116, 3175, 5330, 5760, 921, 517, 7313, 5377, 7018, 54, 3, 3093, 46, 1370, 5859, 1865, 7096, 1544, 7245, 54, 3, 1435, 1457, 6022, 4832, 6812, 7784, 1546, 5585, 2186, 6023, 5, 3, 1577, 4213, 2936, 2229, 6855, 258, 3, 3093, 3612, 46, 1435, 5130, 7848, 6386, 5400, 54, 3, 3166, 5405, 6855, 46, 3257, 5760, 5112, 6999, 7126, 46, 2892, 7126, 46, 2068, 6999, 7126, 2923, 5439, 1235, 6999, 7126, 7096, 6022, 4737, 6999, 7126, 3803, 7788, 3803, 6999, 7126, 2923, 6855, 54, 3, 4012, 6398, 2265, 1225, 6844, 258, 3, 3097, 46, 517, 7028, 6999, 7126, 46, 1235, 6999, 7126, 46, 4737, 6999, 7126, 3803, 7848, 54, 3, 1185, 5377, 2604, 4209, 5850, 517, 492, 492, 54, 3, 517, 7898, 5340, 6060, 629, 18, 3, 1435, 781, 2872, 3862, 6198, 3245, 5330, 4204, 7088, 5591, 258, 3]\n",
      "token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "decoded tokens: ['[CLS]', '▁그', '게', '▁너', '한테', '▁문제', '되지', '▁않았', '으면', '▁좋', '겠', '어', '.', '[SEP]', '▁아니', ',', '▁근', '데', '▁난', '▁문자', '로', '▁사진을', '▁받을', '▁수도', '▁보', '낼', '▁수도', '▁없어', '...', '진짜', '▁', '별로', '야', '.', '[SEP]', '▁네', '가', '▁시간이', '▁나', '고', '▁잠', '깐', '▁얘기', '하고', '▁', '싶', '으면', '▁알려', '줘', ',', '▁나', '▁시간', '▁좀', '▁있어', '...', '[SEP]', '▁방', '금', '▁영화', '관', '에', '▁도착', '했', '어', '.', '[SEP]', '▁알', '겠', '어', ',', '▁걱정', '▁마', '.', '[SEP]', '▁안', '녕', '▁', '^', '_', '^', '[SEP]', '▁안', '녕', ',', '▁잘', '▁지내', '?', '[SEP]', '▁괜찮', '아', '.', '▁방', '금', '▁전에', '▁집', '에', '▁', '왔', '어', '.', '[SEP]', '▁알', '겠', '어', ',', '▁나', '▁지금', '▁고객들', '과', '▁저녁', '▁먹고', '▁있어', '.', '▁네', '가', '▁여기', '▁있으면', '▁좋겠다', '.', '[SEP]', '▁아', '▁알', '겠', '어', ',', '▁미', '안', ',', '▁너', '▁방', '해', '하고', '▁', '싶', '지', '▁않아', '!', '▁나', '도', '▁', '그래', '.', '[SEP]', '▁아니', '야', '▁괜찮', '아', ',', '▁너', '▁생각', '하고', '▁있어', '.', '[SEP]', '▁난', '▁항상', '▁네', '▁문자', '▁받는', '▁게', '▁기대', '돼', '.', '[SEP]', '▁정말', '이', '야', '?', '▁나는', '▁네', '가', '▁바', '쁘', '니까', '▁내가', '▁귀', '찮', '게', '▁하는', '▁것', '▁같아', '▁느껴', '져', '.', '[SEP]', '▁응', '▁항상', '!', '▁나', '▁바', '쁘', '지만', '▁네', '▁문자', '▁보는', '▁게', '▁항상', '▁기대', '돼', '.', '[SEP]', '▁아', ',', '▁너', '▁정말', '▁다', '정', '하다', '▁:', ')', '[SEP]', '▁그냥', '▁', '솔', '직', '하게', '▁말', '하는', '▁거', '야', '.', '[SEP]', '▁너', '▁완벽한', '▁남자', '▁같아', '.', '[SEP]', '▁그렇게', '▁말해', '줘', '서', '▁고', '마', '워', '.', '▁그냥', '▁나', '답', '게', '▁있는', '▁거', '야', '.', '▁난', '▁네', '가', '▁정말', '▁좋아', '.', '[SEP]', '▁나', '도', '▁너', '▁좋아', '해', '.', '▁너', '는', '▁내가', '▁정말', '▁가치', '▁있는', '▁사람', '처럼', '▁느끼', '게', '▁해', '줘', '...', '[SEP]', '▁가치', '▁있다고', '?', '▁세상', '에', ',', '▁너', '▁진짜', '▁', '멋', '져', '.', '▁너', '를', '▁알아', '가', '는', '▁게', '▁', '즐', '거', '워', '.', '[SEP]', '▁아', ',', '▁나', '도', '▁똑같', '이', '▁느껴', '져', '.', '[SEP]', '▁내가', '▁너', '랑', '▁편', '안', '하게', '▁느끼', '길', '▁바', '래', '!', '[SEP]', '▁다음', '▁주', '▁스케줄', '▁받았', '어', '?', '[SEP]', '▁아', '▁응', ',', '▁내가', '▁확인', '해', '볼', '게', '.', '[SEP]', '▁알', '겠', '어', ',', '▁엄마', '는', '▁화', '요', '일', ',', '▁수요', '일', ',', '▁목', '요', '일', '▁쉬', '고', '▁금', '요', '일', '이', '랑', '▁토', '요', '일', '▁일', '하고', '▁일', '요', '일', '▁쉬', '어', '.', '[SEP]', '▁전', '부', '▁밤', '▁근무', '야', '?', '[SEP]', '▁아니', ',', '▁', '월', '요', '일', ',', '▁금', '요', '일', ',', '▁토', '요', '일', '▁일', '해', '.', '[SEP]', '▁그', '거', '▁사실', '▁좋은', '데', '▁', 'ᄏ', 'ᄏ', '.', '[SEP]', '▁', '헷', '갈', '려', '▁:', '(', '[SEP]', '▁내가', '▁갈', '▁수', '▁있다', '면', '▁언제', '가', '▁좋', '을', '까', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(data):\n",
    "    return tokenizer(\n",
    "        data['text'],\n",
    "        add_special_tokens=False,   # 이미 [CLS], [SEP] 추가됨\n",
    "        max_length=512,             # 문장 최대 길이\n",
    "        truncation=True,            # 문장이 max_length보다 길면 자름\n",
    "        padding=True                # 문장이 max_length보다 짧으면 padding\n",
    "    )\n",
    "\n",
    "# 첫 번째 텍스트에 대해 토큰화 수행\n",
    "tokenized_output = tokenize_function(train_data.iloc[0])\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"input_ids: {tokenized_output['input_ids']}\")\n",
    "print(f\"token_type_ids: {tokenized_output['token_type_ids']}\")\n",
    "print(f\"attention_mask: {tokenized_output['attention_mask']}\")\n",
    "print(f\"decoded tokens: {tokenizer.convert_ids_to_tokens(tokenized_output['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8250, 2) (1034, 2) (1024, 2)\n"
     ]
    }
   ],
   "source": [
    "# 검증(Vaildation) 데이터셋 분리\n",
    "print(train_data.shape, valid_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/8250 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8250/8250 [00:04<00:00, 1681.39 examples/s]\n",
      "Map: 100%|██████████| 1034/1034 [00:00<00:00, 1788.30 examples/s]\n",
      "Map: 100%|██████████| 1024/1024 [00:00<00:00, 1577.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Dataset 생성\n",
    "train_dataset = Dataset.from_pandas(train_data) # pandas DataFrame -> Hugging Face Dataset 형식으로 변환\n",
    "valid_dataset = Dataset.from_pandas(valid_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "datasets = DatasetDict({'train': train_dataset, 'valid': valid_dataset, 'test': test_dataset}) # train, valid, test 데이터셋을 묶어서 저장\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True) # train, vaild, test 데이터셋에 tokenize_function 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 8250\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1034\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1024\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 경로 설정\n",
    "model_save_path = '../models/model_save'\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_save_path,                 # 학습 결과 저장 경로\n",
    "    report_to='wandb',                          # wandb 사용\n",
    "    num_train_epochs=15,                        # 학습 epoch 설정\n",
    "    per_device_train_batch_size=32,             # train batch_size 설정\n",
    "    per_device_eval_batch_size=32,              # test batch_size 설정\n",
    "    logging_dir=model_save_path+'/logs',        # 학습 log 저장 경로\n",
    "    logging_steps=100,                          # 학습 log 기록 단위\n",
    "    save_total_limit=2,                         # 학습 결과 저장 최대 개수\n",
    "    evaluation_strategy=\"epoch\",                # 매 epoch마다 평가 실행\n",
    "    save_strategy=\"epoch\",                      # 매 epoch마다 모델 저장\n",
    "    load_best_model_at_end=True,                # 가장 성능이 좋은 모델을 마지막에 load\n",
    ")\n",
    "\n",
    "# 최적화 알고리즘(optimizer) 설정\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# # 스케줄러(scheduler) 설정\n",
    "# scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=len(tokenized_datasets['train']) * training_args.num_train_epochs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 평가 지표 설정(binary classification)\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 생성\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['valid'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델 경로\n",
    "model_checkpoint = \"../models/kobert_finetuning\"\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "model = BertForSequenceClassification.from_pretrained(model_checkpoint).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.13740025460720062,\n",
       " 'eval_model_preparation_time': 0.002,\n",
       " 'eval_accuracy': 0.9755859375,\n",
       " 'eval_f1': 0.9856569133677567,\n",
       " 'eval_precision': 0.983963344788087,\n",
       " 'eval_recall': 0.9873563218390805,\n",
       " 'eval_runtime': 10.6301,\n",
       " 'eval_samples_per_second': 96.33,\n",
       " 'eval_steps_per_second': 3.01}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트(Test) 데이터셋 평가\n",
    "trainer.evaluate(tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding Window('\\t' ver.)\n",
    "def preprocess(texts):\n",
    "    text = \"[CLS] \" + \" \".join(str(m) + \" [SEP]\" for m in texts)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 대화 테스트\n",
    "def predict(chat):\n",
    "    model.eval()\n",
    "    tokenized_sent = tokenizer(\n",
    "        chat,\n",
    "        add_special_tokens=False,   # 이미 [CLS], [SEP] 추가됨\n",
    "        max_length=512,             # 문장 최대 길이\n",
    "        truncation=True,            # 문장이 max_length보다 길면 자름\n",
    "        padding=True,               # 문장이 max_length보다 짧으면 padding\n",
    "        return_tensors='pt'         # PyTorch tensor로 반환\n",
    "    )\n",
    "    tokenized_sent.to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=tokenized_sent[\"input_ids\"],\n",
    "            attention_mask=tokenized_sent[\"attention_mask\"],\n",
    "            token_type_ids=tokenized_sent[\"token_type_ids\"]\n",
    "            )\n",
    "        \n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu()\n",
    "    result = logits.argmax(-1)  \n",
    "    \n",
    "    if result == 0:\n",
    "        return '일상 대화 😇'\n",
    "    elif result == 1:\n",
    "        return '그루밍 대화 👿'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "아 미안해\n",
      "[Pred]: 그루밍 대화 👿\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "아 미안해\n",
      "그래 자중해~\n",
      "[Pred]: 그루밍 대화 👿\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "아 미안해\n",
      "그래 자중해~\n",
      "나 공부하고 올게!!\n",
      "[Pred]: 그루밍 대화 👿\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "아 미안해\n",
      "그래 자중해~\n",
      "나 공부하고 올게!!\n",
      "그래 파이팅!!\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "아 미안해\n",
      "그래 자중해~\n",
      "나 공부하고 올게!!\n",
      "그래 파이팅!!\n",
      "오키~\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "아 미안해\n",
      "그래 자중해~\n",
      "나 공부하고 올게!!\n",
      "그래 파이팅!!\n",
      "오키~\n",
      "잠만 가기 전에 다리 사진 한번만 보내줘~\n",
      "[Pred]: 그루밍 대화 👿\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "아 미안해\n",
      "그래 자중해~\n",
      "나 공부하고 올게!!\n",
      "그래 파이팅!!\n",
      "오키~\n",
      "잠만 가기 전에 다리 사진 한번만 보내줘~\n",
      "아 안돼~\n",
      "[Pred]: 그루밍 대화 👿\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "아 미안해\n",
      "그래 자중해~\n",
      "나 공부하고 올게!!\n",
      "그래 파이팅!!\n",
      "오키~\n",
      "잠만 가기 전에 다리 사진 한번만 보내줘~\n",
      "아 안돼~\n",
      "지금 너무 꼴려서..\n",
      "[Pred]: 일상 대화 😇\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "아 미안해\n",
      "그래 자중해~\n",
      "나 공부하고 올게!!\n",
      "그래 파이팅!!\n",
      "오키~\n",
      "잠만 가기 전에 다리 사진 한번만 보내줘~\n",
      "아 안돼~\n",
      "지금 너무 꼴려서..\n",
      "미안해..\n",
      "[Pred]: 그루밍 대화 👿\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "아 미안해\n",
      "그래 자중해~\n",
      "나 공부하고 올게!!\n",
      "그래 파이팅!!\n",
      "오키~\n",
      "잠만 가기 전에 다리 사진 한번만 보내줘~\n",
      "아 안돼~\n",
      "지금 너무 꼴려서..\n",
      "미안해..\n",
      "그럼 담에 다리 찍어서 보내주기다~\n",
      "[Pred]: 그루밍 대화 👿\n",
      "\n",
      "\n",
      "[Chat]:\n",
      "오늘 뭐했어?\n",
      "오늘 그냥 학교 갔다 왔어\n",
      "오빠는?\n",
      "나도 오늘 그냥 집에서 쉬었어\n",
      "근데 너 생각 많이 났어\n",
      "왜?ㅎㅎㅎ\n",
      "키스하고 싶어서\n",
      "아 뭐래~\n",
      "왜 키스가 어때서\n",
      "나 아직 미성년자야\n",
      "아 미안해\n",
      "그래 자중해~\n",
      "나 공부하고 올게!!\n",
      "그래 파이팅!!\n",
      "오키~\n",
      "잠만 가기 전에 다리 사진 한번만 보내줘~\n",
      "아 안돼~\n",
      "지금 너무 꼴려서..\n",
      "미안해..\n",
      "그럼 담에 다리 찍어서 보내주기다~\n",
      "알았어~\n",
      "[Pred]: 그루밍 대화 👿\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = [\n",
    "    '오늘 뭐했어?',\n",
    "    '오늘 그냥 학교 갔다 왔어',\n",
    "    '오빠는?',\n",
    "    '나도 오늘 그냥 집에서 쉬었어',\n",
    "    '근데 너 생각 많이 났어',\n",
    "    '왜?ㅎㅎㅎ',\n",
    "    '키스하고 싶어서',\n",
    "    '아 뭐래~',\n",
    "    '왜 키스가 어때서',\n",
    "    '나 아직 미성년자야',\n",
    "    '아 미안해',\n",
    "    '그래 자중해~',\n",
    "    '나 공부하고 올게!!',\n",
    "    '그래 파이팅!!',\n",
    "    '오키~',\n",
    "    '잠만 가기 전에 다리 사진 한번만 보내줘~',\n",
    "    '아 안돼~',\n",
    "    '지금 너무 꼴려서..',\n",
    "    '미안해..',\n",
    "    '그럼 담에 다리 찍어서 보내주기다~',\n",
    "    '알았어~'\n",
    "]\n",
    "\n",
    "\n",
    "for i in range(len(test)):\n",
    "    print(f'[Chat]:')\n",
    "    for j in test[:i+1]:\n",
    "        print(j)\n",
    "    print(f'[Pred]: {predict(preprocess(test[:i+1]))}\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
